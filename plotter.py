import logging
import re

import matplotlib.pyplot as plt

logging.root.level = logging.INFO
from IPython.core.interactiveshell import InteractiveShell

InteractiveShell.ast_node_interactivity = "all"
import matplotlib

matplotlib.use('TkAgg')
import pandas as pd

matplotlib.use('TkAgg')
from sklearn.decomposition import PCA
from gensim.models import Word2Vec
from wordcloud import WordCloud


def print_word_cloud_ar(artext_list, word_frequency):
    """Takes a list of Arabic words to print cloud."""
    full_string = ' '.join(artext_list)
    artext = full_string
    # Build the word cloud
    wordc = WordCloud(font_path='tahoma', background_color='black', width=1920, height=1080).generate(artext)
    wordc.fit_words(word_frequency)

    # Draw the word cloud
    plt.imshow(wordc)
    plt.axis("off")
    plt.tight_layout(pad=0)

    plt.show()


def print_similar_word_cloud(one_word, topn):
    """Takes an word and print similar word cloud for top number of words {$topn}."""
    temp_tuple = model.most_similar(positive=[one_word], negative=[], topn=topn)
    similar_words = [i[0] for i in temp_tuple]
    word_frequency = {}
    for word_tuple in temp_tuple:
        reshaped_word = word_tuple[0]
        key = reshaped_word
        word_frequency[key] = word_tuple[1]
    print(temp_tuple)

    print_word_cloud_ar(similar_words, word_frequency)


###################
# Parameter Tuning
###################
# min_count is for pruning the internal dictionary.
# Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage.
min_count = 10
# The Word2Vec Skip-gram model, for example,
# takes in pairs (word1, word2) generated by moving a window across text data,
# and trains a 1-hidden-layer neural network based on the synthetic task of given an input word,
# giving us a predicted probability distribution of nearby words to the input.
window = 5
# workers , the last of the major parameters (full list here) is for training parallelization, to speed up training:
workers = 10
# learning rate
alpha = 0.22

# Start of program
try:
    # Load Quran from csv into a dataframe
    d = pd.read_csv('data\quran.csv')
except:
    print('Failed to load the quran file, make sure you are in the correct dir "data\quran.csv"')

if (len(d) > 0):
    print("print total rows count: ")
    print(len(d))
    # print("print total data before NaN removal: ")
    # print(d)
    d.fillna('', inplace=True)
    concoc = d['Surah'].astype(str) + "-" + d['Ayah'].astype(str)

    # print("print total data after NaN removal: " )
    # print(d)
    # print(concoc)
    concoc = pd.DataFrame(concoc)
    dropped_column = d.drop(d.columns[-1], axis=1)  # Drop last column
    print("print total data After last column removal: ")
    print(dropped_column)
    dropped_column = pd.DataFrame(dropped_column)
    # print(dropped_column)
    # print(dropped_column.head()) #if you want to view first n-rows
    # print(dropped_column.tail())  # if you want to view last  n-rows
    # print(dropped_column[20:30 + 1])  #slice and dice of rows from 20 : 30
    # print(dropped_column.describe()) #Statistics of your dataframe if value is int

# spliting each word
if range(len(dropped_column['Text'])):
    dropped_column['Text'] = dropped_column['Text'].str.split()
print("Splitting each word in Text : ")
print(dropped_column)

SurahAyah = concoc.values.tolist()
print(SurahAyah)
for i in range(len(SurahAyah)):
    SurahAyah[i] = [number.lower() for number in SurahAyah[i] if re.match('^[0-9-]+', number)]
print(SurahAyah)
# You can filter for one surah too if you want!
verses = dropped_column['Text'].values.tolist()
print(verses)
# regex and non usable character check and replace
for i in range(len(verses)):
    verses[i] = [word.lower() for word in verses[i] if re.match('^[a-zA-Z]+', word)]
    verses[i] = [word.replace('.', '') for word in verses[i]]
    verses[i] = [word.replace(';', '') for word in verses[i]]
    verses[i] = [word.replace('(', '') for word in verses[i]]
    verses[i] = [word.replace(')', '') for word in verses[i]]
    verses[i] = [word.replace(':', '') for word in verses[i]]
    verses[i] = [word.replace('\'', '') for word in verses[i]]
    verses[i] = [word.replace('?', '') for word in verses[i]]

# Creating a model verses
model = Word2Vec(verses, min_count=min_count, window=window, workers=workers, alpha=alpha)
model.wv.vocab
model.save("quran.bin")
X = model[model.wv.vocab]
pca = PCA(n_components=2)
pca.fit(X)
result = pca.fit_transform(X)
# create a scatter plot of the projection
plt.scatter(result[:, 0], result[:, 1])
words = list(model.wv.vocab)
for i, word in enumerate(words):
    plt.annotate(word, xy=(result[i, 0], result[i, 1]))
